import pandas as pd
import spacy
from tqdm import tqdm
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer, util

nlp = spacy.load("en_core_web_lg")
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Step 1: åŠ è½½äº§å“è¯å…¸
# ------------------------
def load_product_dictionary(dict_path: str):
    product_df = pd.read_csv(dict_path)
    # alias_to_product: alias(è‹±æ–‡å) -> (ä¸­æ–‡äº§å“å, alias)
    alias_to_product = {}
    for _, row in product_df.iterrows():
        product_name = row["äº§å“å"]
        aliases = str(row["è‹±æ–‡"]).split(",")
        for alias in aliases:
            alias = alias.strip().lower()
            if alias:
                alias_to_product[alias] = (product_name, alias)

    return alias_to_product

# Step 2: æŸ¥æ‰¾äº§å“ä¸ä¸Šä¸‹æ–‡
# ------------------------
kw_model = KeyBERT(model='all-MiniLM-L6-v2')  

def find_products_with_context(text, alias_to_product):
    doc = nlp(text)
    matches = []

    for sent in doc.sents:
        sent_text = sent.text.lower()
        sent_start = sent.start_char
        sent_end = sent.end_char

        # åœ¨å¥å­ä¸­æŸ¥æ‰¾æ‰€æœ‰alias
        for alias, (product_name, _) in alias_to_product.items():
            if alias in sent_text:
                keywords = kw_model.extract_keywords(
                    sent.text, 
                    keyphrase_ngram_range=(1, 4),
                    stop_words='english', # å¯è‡ªå®šä¹‰
                    top_n=5
                )
                matches.append((
                    product_name,  # äº§å“å
                    alias,         # åŒ¹é…åˆ«å
                    sent_start,    # å¥å­èµ·å§‹å­—ç¬¦ç´¢å¼•
                    sent_end,      # å¥å­ç»“æŸå­—ç¬¦ç´¢å¼•
                    sent.text,     # åŸå§‹å¥å­æ–‡æœ¬
                    [k[0] for k in keywords]  # å…³é”®è¯åˆ—è¡¨
                ))
    return matches

# Step 3: æ„å»ºäº§å“ç»„åˆ
# ------------------------
def remove_overlapping_matches(matches):
    matches.sort(key=lambda x: x[2])  # æŒ‰èµ·å§‹å­—ç¬¦ç´¢å¼•æ’åº
    grouped = []
    current_group = []

    for match in matches:
        if not current_group:
            current_group.append(match)
            continue
        last = current_group[-1]
        if match[2] <= last[3]:  # å½“å‰å¥å­èµ·å§‹ <= ä¸Šä¸€å¥å­ç»“æŸï¼Œè¯´æ˜é‡å 
            current_group.append(match)
        else:
            grouped.append(current_group)
            current_group = [match]

    if current_group:
        grouped.append(current_group)

    final = []
    for group in grouped:
        if len(group) == 1:
            final.append(group[0])
        else:
            products = sorted(set([m[0] for m in group]))
            aliases = sorted(set([m[1] for m in group]))
            # å–ç¬¬ä¸€ä¸ªå¥å­çš„èµ·æ­¢å’Œæ–‡æœ¬
            start_pos = group[0][2]
            end_pos = group[-1][3]
            context_text = " ".join([m[4] for m in group])  # åˆå¹¶æ‰€æœ‰å¥å­æ–‡æœ¬ä½œä¸ºç»„åˆè¯­ä¹‰ç¾¤ä¸Šä¸‹æ–‡
            keywords_combined = []
            for m in group:
                keywords_combined.extend(m[5])
            keywords_combined = list(set(keywords_combined))  # å»é‡å…³é”®è¯

            final.append((
                "+".join(products),
                ",".join(aliases),
                start_pos,
                end_pos,
                context_text,
                keywords_combined
            ))

    return final

# Step 4: æŠ½å–è¯­ä¹‰ç¾¤ç»“æ„
# ------------------------
def extract_product_semantic_groups(text, alias_to_product):
    raw_matches = find_products_with_context(text, alias_to_product)
    filtered_matches = remove_overlapping_matches(raw_matches)

    product_contexts = {}

    for prod_name, alias, _, _, context, keywords in filtered_matches:
        products = prod_name.split("+")
        aliases = alias.split(",") if isinstance(alias, str) else [alias]

        if len(products) == 1:
            prod = products[0]
            if prod not in product_contexts:
                product_contexts[prod] = {"aliases": set(), "contexts": [], "semantic_groups": []}
            product_contexts[prod]["aliases"].update(aliases)
            if context not in product_contexts[prod]["contexts"]:
                product_contexts[prod]["contexts"].append(context)
            if keywords not in product_contexts[prod]["semantic_groups"]:
                product_contexts[prod]["semantic_groups"].append(keywords)
        else:
            combo_key = "+".join(products)
            if combo_key not in product_contexts:
                product_contexts[combo_key] = {"aliases": set(), "contexts": [], "semantic_groups": []}
            product_contexts[combo_key]["aliases"].update(aliases)
            if context not in product_contexts[combo_key]["contexts"]:
                product_contexts[combo_key]["contexts"].append(context)
            if keywords not in product_contexts[combo_key]["semantic_groups"]:
                product_contexts[combo_key]["semantic_groups"].append(keywords)

    # è½¬ä¸º list
    for prod in product_contexts:
        product_contexts[prod]["aliases"] = list(product_contexts[prod]["aliases"])
    return product_contexts

# Step 5: ä¸»å‡½æ•°ï¼Œä¼ å…¥æ–‡æœ¬å’Œè¯å…¸è·¯å¾„
# ------------------------
def match_texts_to_products(text_df: pd.DataFrame, text_column: str, dict_path: str):
    print("ğŸ”„ åŠ è½½äº§å“è¯å…¸...")
    alias_to_product = load_product_dictionary(dict_path)

    tqdm.pandas(desc="ğŸ” åˆ†æä¸­")
    text_df["äº§å“è¯­ä¹‰ç»“æ„"] = text_df[text_column].progress_apply(
        lambda x: extract_product_semantic_groups(x, alias_to_product)
    )

    output = []
    for _, row in text_df.iterrows():
        url = row["url"] 
        prod_dict = row["äº§å“è¯­ä¹‰ç»“æ„"]
        for prod_name, data in prod_dict.items():
            output.append([url, prod_name, data["aliases"], data["contexts"]])
    return output

# Step 6: è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
# ------------------------
def assign_needs_by_semantic_similarity(result_df, label_embeddings, need_names, threshold=0.05):
    matched_records = []

    for _, row in tqdm(result_df.iterrows(), total=len(result_df), desc="ğŸ” åˆ†æä¸­"):
        semantic_group = row["è¯­ä¹‰ç¾¤"]
        if isinstance(semantic_group, list):
            sem_text = " ".join(semantic_group)
        else:
            sem_text = str(semantic_group)

        sem_embedding = embedding_model.encode(sem_text, convert_to_tensor=True)

        cos_scores = util.cos_sim(sem_embedding, label_embeddings)[0]

        matched_needs = {
            need_names[i]: round(score.item(), 2)
            for i, score in enumerate(cos_scores)
            # if score >= threshold
        }

        matched_records.append({
            "url": row["url"],
            "äº§å“å": row["äº§å“å"],
            "è‹±æ–‡": row["è‹±æ–‡"],
            "è¯­ä¹‰ç¾¤": semantic_group,
            "åŒ¹é…éœ€æ±‚": matched_needs
        })

    return pd.DataFrame(matched_records)

# Step 7: é‡æ„è®­ç»ƒè¾“å…¥
# ------------------------
def restructure_training_input(need_df: pd.DataFrame):
    rows = []

    for _, row in need_df.iterrows():
        product = row["äº§å“å"]
        semantic_group = ", ".join(row["è¯­ä¹‰ç¾¤"]) if isinstance(row["è¯­ä¹‰ç¾¤"], list) else str(row["è¯­ä¹‰ç¾¤"])
        label_scores = row["åŒ¹é…éœ€æ±‚"]  # æ˜¯ä¸€ä¸ª dict: {label: score}

        for label, score in label_scores.items():
            rows.append({
                "product": product,
                "semantic_group": semantic_group,
                "label": label,
                "score": round(score, 4),
                "manual_score": None
            })

    return pd.DataFrame(rows)


   